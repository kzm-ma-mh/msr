"""
QA Dataset Generator
ÙØ±Ù…Øª Alpaca: instruction, input, output

Ø§Ø² Issues Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒØ´Ù‡: Ø³ÙˆØ§Ù„ (issue) â†’ Ø¬ÙˆØ§Ø¨ (comments/resolution)
"""

import json
import os
import re
import requests
from tqdm import tqdm
from config import Config


class QADatasetGenerator:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Config.GITEA_HEADERS)
        self.dataset = []

    def _api(self, endpoint, params=None):
        """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Gitea API"""
        url = f"{Config.GITEA_API}{endpoint}"
        resp = self.session.get(url, params=params)
        if resp.status_code == 200:
            return resp.json()
        return None

    def _get_all_issues(self):
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Issues (ÙÙ‚Ø· issue Ù‡Ø§ØŒ Ù†Ù‡ PRÙ‡Ø§)"""
        print("\nğŸ“¥ Fetching all issues from Gitea...")
        all_issues = []
        page = 1

        while True:
            endpoint = f"/repos/{Config.GITEA_ORG}/{Config.REPO_NAME}/issues"
            params = {
                "type": "issues",
                "state": "all",
                "limit": 50,
                "page": page,
            }
            data = self._api(endpoint, params)

            if not data:
                break

            for issue in data:
                title = issue.get("title", "")
                if not title.startswith("[PR #"):
                    all_issues.append(issue)

            if len(data) < 50:
                break

            page += 1

        print(f"   Found {len(all_issues)} issues (excluding PRs)")
        return all_issues

    def _get_issue_comments(self, issue_number):
        """Ø¯Ø±ÛŒØ§ÙØª Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© Issue"""
        endpoint = f"/repos/{Config.GITEA_ORG}/{Config.REPO_NAME}/issues/{issue_number}/comments"
        return self._api(endpoint) or []

    def _clean_text(self, text):
        """ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†"""
        if not text:
            return ""
        text = re.sub(r'\n---\nğŸ“Œ.*$', '', text, flags=re.DOTALL)
        return text.strip()

    def _extract_code_blocks(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ú©Ø¯"""
        blocks = re.findall(r'```[\w]*\n(.*?)```', text, re.DOTALL)
        return blocks

    def _has_code(self, text):
        """Ø¢ÛŒØ§ Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø¯ Ù‡Ø³ØªØŸ"""
        return bool(re.search(r'```', text))

    def generate_question_answer(self):
        """
        ØªÙˆÙ„ÛŒØ¯ Ø³ÙˆØ§Ù„-Ø¬ÙˆØ§Ø¨ Ø§Ø² Issues
        Issue title/body = Ø³ÙˆØ§Ù„
        Comments = Ø¬ÙˆØ§Ø¨
        """
        print("\nâ“ Generating QA pairs from issues...")

        issues = self._get_all_issues()

        for issue in tqdm(issues, desc="   Processing issues"):
            title = issue.get("title", "")
            body = self._clean_text(issue.get("body", ""))
            issue_number = issue.get("number")
            labels = [l.get("name", "") for l in issue.get("labels", [])]

            if not title or len(title) < 10:
                continue

            comments = self._get_issue_comments(issue_number)
            if not comments:
                continue

            # Ø³Ø§Ø®Øª Ø³ÙˆØ§Ù„
            clean_title = re.sub(r'^\[#\d+\]\s*', '', title).strip()
            question = clean_title
            if body and len(body) > 20:
                question += f"\n\nDetails:\n{body[:2000]}"

            # Ø³Ø§Ø®Øª Ø¬ÙˆØ§Ø¨ Ø§Ø² Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§
            answer_parts = []
            for comment in comments:
                comment_body = comment.get("body", "")
                if not comment_body or len(comment_body) < 20:
                    continue
                comment_body = re.sub(r'^\*\*@.*?\*\*.*?:\n\n', '', comment_body)
                answer_parts.append(comment_body.strip())

            if not answer_parts:
                continue

            best_answer = answer_parts[0]
            full_answer = "\n\n---\n\n".join(answer_parts[:5])

            # Ù†ÙˆØ¹ Û±: Ø³ÙˆØ§Ù„ â†’ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¬ÙˆØ§Ø¨
            self.dataset.append({
                "instruction": question,
                "input": f"Repository: FastAPI\nLabels: {', '.join(labels)}",
                "output": best_answer[:5000],
                "source": "issue",
                "type": "qa_best_answer",
                "issue_number": issue_number,
            })

            # Ù†ÙˆØ¹ Û²: Ø³ÙˆØ§Ù„ â†’ Ø¬ÙˆØ§Ø¨ Ú©Ø§Ù…Ù„
            if len(answer_parts) > 1:
                self.dataset.append({
                    "instruction": f"Provide a comprehensive answer to this question: {clean_title}",
                    "input": body[:2000] if body else "No additional context",
                    "output": full_answer[:8000],
                    "source": "issue",
                    "type": "qa_comprehensive",
                    "issue_number": issue_number,
                })

            # Ù†ÙˆØ¹ Û³: QA ØªÚ©Ù†ÛŒÚ©Ø§Ù„ (Ø§Ú¯Ù‡ Ú©Ø¯ Ø¯Ø§Ø±Ù‡)
            if self._has_code(body) or any(self._has_code(a) for a in answer_parts):
                code_blocks_q = self._extract_code_blocks(body)
                code_blocks_a = []
                for a in answer_parts:
                    code_blocks_a.extend(self._extract_code_blocks(a))

                if code_blocks_q or code_blocks_a:
                    self.dataset.append({
                        "instruction": f"Debug and solve this technical issue: {clean_title}",
                        "input": body[:3000] if body else question,
                        "output": best_answer[:5000],
                        "source": "issue",
                        "type": "qa_technical",
                        "issue_number": issue_number,
                    })

    def generate_from_closed_issues(self):
        """
        ØªÙˆÙ„ÛŒØ¯ QA Ø§Ø² issue Ù‡Ø§ÛŒ Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù‡
        """
        print("\nâœ… Generating QA from closed/resolved issues...")

        issues = self._get_all_issues()
        closed_issues = [i for i in issues if i.get("state") == "closed"]

        print(f"   Found {len(closed_issues)} closed issues")

        for issue in tqdm(closed_issues, desc="   Closed issues"):
            title = issue.get("title", "")
            body = self._clean_text(issue.get("body", ""))
            issue_number = issue.get("number")

            clean_title = re.sub(r'^\[#\d+\]\s*', '', title).strip()

            comments = self._get_issue_comments(issue_number)

            if not comments:
                continue

            last_comment = comments[-1].get("body", "")
            last_comment = re.sub(r'^\*\*@.*?\*\*.*?:\n\n', '', last_comment)

            if len(last_comment) < 30:
                continue

            self.dataset.append({
                "instruction": f"How was this issue resolved: {clean_title}",
                "input": body[:2000] if body else "No additional context provided",
                "output": f"Resolution:\n{last_comment[:5000]}",
                "source": "issue_closed",
                "type": "qa_resolution",
                "issue_number": issue_number,
            })

    def generate_how_to(self):
        """
        ØªÙˆÙ„ÛŒØ¯ How-to Ø§Ø² issues Ø¨Ø§ Ø³ÙˆØ§Ù„Ø§Øª how/what/why
        """
        print("\nğŸ“– Generating How-to QA pairs...")

        issues = self._get_all_issues()

        keywords = ["how", "what", "why", "can i", "is it possible",
                     "does", "should", "best way", "example", "help"]

        for issue in tqdm(issues, desc="   How-to issues"):
            title = issue.get("title", "")
            clean_title = re.sub(r'^\[#\d+\]\s*', '', title).strip().lower()
            body = self._clean_text(issue.get("body", ""))
            issue_number = issue.get("number")

            is_question = any(kw in clean_title for kw in keywords)
            if not is_question:
                continue

            comments = self._get_issue_comments(issue_number)
            if not comments:
                continue

            best_comments = []
            for c in comments:
                c_body = c.get("body", "")
                c_body = re.sub(r'^\*\*@.*?\*\*.*?:\n\n', '', c_body)
                if len(c_body) > 30:
                    best_comments.append(c_body)

            if not best_comments:
                continue

            answer = "\n\n".join(best_comments[:3])

            self.dataset.append({
                "instruction": re.sub(r'^\[#\d+\]\s*', '', title).strip(),
                "input": body[:2000] if body else "Context: FastAPI web framework",
                "output": answer[:5000],
                "source": "issue",
                "type": "how_to",
                "issue_number": issue_number,
            })

    def generate(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ Ø¯ÛŒØªØ§Ø³Øª QA"""
        print("=" * 60)
        print("â“ QA DATASET GENERATOR")
        print("=" * 60)

        self.generate_question_answer()
        self.generate_from_closed_issues()
        self.generate_how_to()

        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        seen = set()
        unique_dataset = []
        for item in self.dataset:
            key = item["instruction"][:100]
            if key not in seen:
                seen.add(key)
                unique_dataset.append(item)
        self.dataset = unique_dataset

        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª JSONL
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        output_path = os.path.join(Config.OUTPUT_DIR, "qa_dataset.jsonl")

        with open(output_path, "w", encoding="utf-8") as f:
            for item in self.dataset:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        print(f"\nâœ… QA Dataset Generated!")
        print(f"   ğŸ“Š Total samples: {len(self.dataset)}")
        print(f"   ğŸ’¾ Saved to: {output_path}")

        types = {}
        for item in self.dataset:
            t = item.get("type", "unknown")
            types[t] = types.get(t, 0) + 1

        print(f"\n   ğŸ“ˆ Breakdown:")
        for t, count in sorted(types.items(), key=lambda x: -x[1]):
            print(f"      {t}: {count}")

        return self.dataset